# -*- coding: utf-8 -*-
"""Optimizing Mushroom Classification through Machine Learning and Hyperparameter Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DSbLr6Fxb1vxTdHnK5ZvwPORYHR8MWXy
"""

!pip install pyspark

"""# **Import Library**"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
import matplotlib.pyplot as plt

spark = SparkSession.builder.appName('Mushroom').getOrCreate()
df = spark.read.csv('/content/mushrooms.csv', header = True, inferSchema = True)
df.show()

"""# **1. Exploratory Data Analysis (EDA) => Data Distribution**"""

# Check the number of records in the dataset
total_records = df.count()
print(f"Total number of records: {total_records}")

# Check the distribution of classes
class_distribution = df.groupBy('class').count()
print("Class distribution:")
class_distribution.show()

df.printSchema()

"""# **2. Data Preprocessing**

# **#1 Handling Missing Value**
"""

# Menghitung dan mencetak jumlah nilai null pada setiap kolom
for colname in df.columns:
    null_count = df.filter(col(colname).isNull()).count()
    print(f" {colname}: {null_count}")

"""# **#2 Handling Duplicated Data**"""

# Menghitung jumlah duplikat data
duplicate_counts = df.groupBy(df.columns).count().filter(col("count") > 1).count()

# Mencetak hasil
print(f"Jumlah duplikat data: {duplicate_counts}")

"""# **#3 Encoding**"""

# Daftar kolom dengan tipe data string
string_columns = [col_name for col_name, col_type in df.dtypes if col_type == 'string']

# Membuat pipeline untuk label encoding
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+"_index", handleInvalid="skip").fit(df) for col_name in string_columns]
pipeline = Pipeline(stages=indexers)
df_encoded = pipeline.fit(df).transform(df)

# Menampilkan DataFrame yang telah diubah
df_encoded.select([col_name+"_index" for col_name in string_columns]).show()

df_encoded.printSchema()

# Menghapus kolom-kolom dengan tipe data string
df = df_encoded.drop(*string_columns)

df.printSchema()

numeric_features = [t[0] for t in df.dtypes if t[1] in ['int','double']]

# yang diambil hanya kolom yang memiliki fitur numerik
numeric_summary = df.select(numeric_features).summary()
numeric_summary.show(truncate=False)

"""# **#4 Handling Imbalance Data (SMOTE)**"""

from imblearn.over_sampling import SMOTE
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Inisialisasi Spark
spark = SparkSession.builder.appName("SMOTEExample").getOrCreate()

# Ambil data dari PySpark DataFrame ke Pandas DataFrame
df_pd = df.toPandas()

# Pisahkan fitur dan target
X = df_pd.drop('class_index', axis=1)
y = df_pd['class_index']

# Terapkan SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Gabungkan kembali fitur dan target menjadi DataFrame
df_resampled_pd = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled_pd['class_index'] = y_resampled

# Konversi kembali ke PySpark DataFrame
df_resampled = spark.createDataFrame(df_resampled_pd)

# Tampilkan jumlah data pada masing-masing kelas setelah oversampling
class_counts_after = df_resampled.groupBy("class_index").count()
print("Class counts after SMOTE:")
class_counts_after.show()

"""# **1. EDA (Correlation Analysis)**"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Daftar kolom dengan tipe data integer atau double
numeric_columns = [col_name for col_name, col_type in df_resampled.dtypes if col_type in ['int', 'double']]

# Membuat VectorAssembler
vector_assembler = VectorAssembler(inputCols=numeric_columns, outputCol="features")

# Menggabungkan kolom-kolom numerik menjadi fitur tunggal
df_assembled = vector_assembler.transform(df_resampled).select("features")

# Menghitung matriks korelasi
matrix = Correlation.corr(df_assembled, "features").head()
correlation_matrix = matrix[0].toArray()

# Membuat DataFrame dari matriks korelasi
correlation_df = pd.DataFrame(correlation_matrix, index=numeric_columns, columns=numeric_columns)

# Menampilkan heatmap menggunakan Seaborn dan Matplotlib
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_df, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Matrix Heatmap (Numeric Columns Only)")
plt.show()

# Menghapus kolom 'veil-type_index' dari DataFrame
df_filtered = df_resampled.drop('veil-type_index')

# Daftar kolom dengan tipe data integer atau double
numeric_columns = [col_name for col_name, col_type in df_filtered.dtypes if col_type in ['int', 'double']]

# Membuat VectorAssembler
vector_assembler = VectorAssembler(inputCols=numeric_columns, outputCol="features")

# Menggabungkan kolom-kolom numerik menjadi fitur tunggal
df_assembled = vector_assembler.transform(df_filtered).select("features")

# Menghitung matriks korelasi
matrix = Correlation.corr(df_assembled, "features").head()
correlation_matrix = matrix[0].toArray()

# Membuat DataFrame dari matriks korelasi
correlation_df = pd.DataFrame(correlation_matrix, index=numeric_columns, columns=numeric_columns)

# Menampilkan heatmap menggunakan Seaborn dan Matplotlib
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_df, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Matrix Heatmap (Numeric Columns Only)")
plt.show()

"""# **#5 Fitur Selection (Pearson Correlation)**"""

# Menentukan ambang korelasi yang dianggap relevan
threshold = 0.1

# Memfilter fitur-fitur yang memiliki korelasi lebih besar dari ambang dengan class_index
relevant_features = [col_name for col_name in correlation_df.index
                     if abs(correlation_df.loc[col_name, 'class_index']) > threshold
                     and col_name != "class_index"]

# Menampilkan 5 fitur teratas yang dianggap relevan
print("5 Fitur Teratas yang Dianggap Relevan terhadap class_index:")
for feature in relevant_features[:5]:
    print(feature)

# Memisahkan DataFrame menjadi fitur dan label
selected_features = ['cap-shape_index', 'cap-surface_index', 'cap-color_index', 'bruises_index', 'odor_index']
assembler = VectorAssembler(inputCols=selected_features, outputCol='features')
df_split = assembler.transform(df_resampled).select('features', 'class_index')

# Tampilkan hasil
df_split.show(truncate=False)

"""# **#6 Splitting Data**"""

# Membagi data menjadi train (50%), test (25%), dan validation (25%)
train, temp = df_split.randomSplit([0.5, 0.5], seed=42)
test, validation = temp.randomSplit([0.5, 0.5], seed=42)

print("Training Dataset Count: " + str(train.count()))
print("Test Dataset Count: " + str(test.count()))
print("Validation Dataset Count: " + str(validation.count()))

"""# **3. Modelling**"""

from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, NaiveBayes
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Inisialisasi model
rf = RandomForestClassifier(featuresCol='features', labelCol='class_index')
lr = LogisticRegression(featuresCol='features', labelCol='class_index')
dt = DecisionTreeClassifier(featuresCol='features', labelCol='class_index')
nb = NaiveBayes(featuresCol='features', labelCol='class_index')

# Fit model pada data latih
rf_model = rf.fit(train)
lr_model = lr.fit(train)
dt_model = dt.fit(train)
nb_model = nb.fit(train)

# Transformasi data uji dan validasi dengan model yang sudah dilatih
rf_test_predictions = rf_model.transform(test)
lr_test_predictions = lr_model.transform(test)
dt_test_predictions = dt_model.transform(test)
nb_test_predictions = nb_model.transform(test)

rf_validation_predictions = rf_model.transform(validation)
lr_validation_predictions = lr_model.transform(validation)
dt_validation_predictions = dt_model.transform(validation)
nb_validation_predictions = nb_model.transform(validation)

"""# **4. Performance Evaluation**"""

from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.functions import vector_to_array
import numpy as np
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Inisialisasi evaluator untuk klasifikasi multi-kelas
evaluator = MulticlassClassificationEvaluator(labelCol='class_index', predictionCol='prediction')

# Fungsi untuk menghitung dan menampilkan metrik
def calculate_metrics(predictions):
    accuracy = evaluator.setMetricName("accuracy").evaluate(predictions)
    precision = evaluator.setMetricName("weightedPrecision").evaluate(predictions)
    recall = evaluator.setMetricName("weightedRecall").evaluate(predictions)
    f1 = evaluator.setMetricName("f1").evaluate(predictions)

    # Menghitung confusion matrix
    confusion_matrix_df = predictions.groupBy('class_index', 'prediction').count().toPandas()
    confusion_matrix_pivot = confusion_matrix_df.pivot(index='class_index', columns='prediction', values='count').fillna(0)

    return confusion_matrix_pivot, accuracy * 100, precision * 100, recall * 100, f1 * 100

# Hitung metrik untuk data validasi
rf_val_metrics = calculate_metrics(rf_validation_predictions)
lr_val_metrics = calculate_metrics(lr_validation_predictions)
dt_val_metrics = calculate_metrics(dt_validation_predictions)
nb_val_metrics = calculate_metrics(nb_validation_predictions)

# Hitung metrik untuk data uji
rf_test_metrics = calculate_metrics(rf_test_predictions)
lr_test_metrics = calculate_metrics(lr_test_predictions)
dt_test_metrics = calculate_metrics(dt_test_predictions)
nb_test_metrics = calculate_metrics(nb_test_predictions)

# Print hasil evaluasi
def print_metrics(model_name, metrics, dataset_type):
    conf_matrix, accuracy, precision, recall, f1 = metrics
    print(f'{model_name} ({dataset_type})')
    print(f'Accuracy: {accuracy:.2f}%')
    print(f'Precision: {precision:.2f}%')
    print(f'Recall: {recall:.2f}%')
    print(f'F1-Score: {f1:.2f}%')
    print()

print_metrics('Random Forest', rf_val_metrics, 'Validation')
print_metrics('Logistic Regression', lr_val_metrics, 'Validation')
print_metrics('Decision Tree', dt_val_metrics, 'Validation')
print_metrics('Naive Bayes', nb_val_metrics, 'Validation')

print_metrics('Random Forest', rf_test_metrics, 'Test')
print_metrics('Logistic Regression', lr_test_metrics, 'Test')
print_metrics('Decision Tree', dt_test_metrics, 'Test')
print_metrics('Naive Bayes', nb_test_metrics, 'Test')

# Inisialisasi evaluator untuk binary classification
binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='class_index')

# Fungsi untuk menghitung ROC dan AUC
def calculate_roc_auc(predictions):
    # Ubah kolom rawPrediction ke array numpy
    pred_array = np.array(predictions.select('rawPrediction').rdd.map(lambda row: row[0].toArray()).collect())
    label_array = np.array(predictions.select('class_index').rdd.map(lambda row: row[0]).collect())

    # Hitung ROC Curve
    fpr, tpr, _ = roc_curve(label_array, pred_array[:, 1])

    # Hitung AUC
    roc_auc = auc(fpr, tpr)

    return fpr, tpr, roc_auc

# Hitung ROC dan AUC untuk data validasi
rf_val_roc_auc = calculate_roc_auc(rf_validation_predictions)
lr_val_roc_auc = calculate_roc_auc(lr_validation_predictions)
dt_val_roc_auc = calculate_roc_auc(dt_validation_predictions)
nb_val_roc_auc = calculate_roc_auc(nb_validation_predictions)

# Hitung ROC dan AUC untuk data uji
rf_test_roc_auc = calculate_roc_auc(rf_test_predictions)
lr_test_roc_auc = calculate_roc_auc(lr_test_predictions)
dt_test_roc_auc = calculate_roc_auc(dt_test_predictions)
nb_test_roc_auc = calculate_roc_auc(nb_test_predictions)

# Print ROC AUC scores as percentages
def print_roc_auc(model_name, roc_auc, dataset_type):
    print(f'{model_name} ROC AUC ({dataset_type}): {roc_auc:.0f}%')

print_roc_auc('Random Forest', rf_val_roc_auc[2] * 100, 'Validation')
print_roc_auc('Logistic Regression', lr_val_roc_auc[2] * 100, 'Validation')
print_roc_auc('Decision Tree', dt_val_roc_auc[2] * 100, 'Validation')
print_roc_auc('Naive Bayes', nb_val_roc_auc[2] * 100, 'Validation')

print_roc_auc('Random Forest', rf_test_roc_auc[2] * 100, 'Test')
print_roc_auc('Logistic Regression', lr_test_roc_auc[2] * 100, 'Test')
print_roc_auc('Decision Tree', dt_test_roc_auc[2] * 100, 'Test')
print_roc_auc('Naive Bayes', nb_test_roc_auc[2] * 100, 'Test')

# Visualisasi confusion matrix
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

# Visualisasi ROC Curve
def plot_roc_curve(fpr, tpr, auc_score, title):
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc_score:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.show()

# Plot confusion matrices
plot_confusion_matrix(rf_val_metrics[0], 'Confusion Matrix Random Forest (Validation)')
plot_confusion_matrix(lr_val_metrics[0], 'Confusion Matrix Logistic Regression (Validation)')
plot_confusion_matrix(dt_val_metrics[0], 'Confusion Matrix Decision Tree (Validation)')
plot_confusion_matrix(nb_val_metrics[0], 'Confusion Matrix Naive Bayes (Validation)')

plot_confusion_matrix(rf_test_metrics[0], 'Confusion Matrix Random Forest (Test)')
plot_confusion_matrix(lr_test_metrics[0], 'Confusion Matrix Logistic Regression (Test)')
plot_confusion_matrix(dt_test_metrics[0], 'Confusion Matrix Decision Tree (Test)')
plot_confusion_matrix(nb_test_metrics[0], 'Confusion Matrix Naive Bayes (Test)')

# Plot ROC Curves
plot_roc_curve(rf_val_roc_auc[0], rf_val_roc_auc[1], rf_val_roc_auc[2], 'ROC Curve Random Forest (Validation)')
plot_roc_curve(lr_val_roc_auc[0], lr_val_roc_auc[1], lr_val_roc_auc[2], 'ROC Curve Logistic Regression (Validation)')
plot_roc_curve(dt_val_roc_auc[0], dt_val_roc_auc[1], dt_val_roc_auc[2], 'ROC Curve Decision Tree (Validation)')
plot_roc_curve(nb_val_roc_auc[0], nb_val_roc_auc[1], nb_val_roc_auc[2], 'ROC Curve Naive Bayes (Validation)')

plot_roc_curve(rf_test_roc_auc[0], rf_test_roc_auc[1], rf_test_roc_auc[2], 'ROC Curve Random Forest (Test)')
plot_roc_curve(lr_test_roc_auc[0], lr_test_roc_auc[1], lr_test_roc_auc[2], 'ROC Curve Logistic Regression (Test)')
plot_roc_curve(dt_test_roc_auc[0], dt_test_roc_auc[1], dt_test_roc_auc[2], 'ROC Curve Decision Tree (Test)')
plot_roc_curve(nb_test_roc_auc[0], nb_test_roc_auc[1], nb_test_roc_auc[2], 'ROC Curve Naive Bayes (Test)')

"""# **5. Hyperparameter Tuning**

:# **1. Logistic Regression & Naive Bayes**
"""

from pyspark.ml.classification import LogisticRegression, NaiveBayes
from pyspark.ml.feature import VectorAssembler, Binarizer
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Menentukan nama kolom fitur yang benar
feature_columns = [col for col in train.columns if col != 'class_index']

# Assembler untuk fitur dengan nama output yang unik
assembler = VectorAssembler(inputCols=feature_columns, outputCol='features_assembled')

# Logistic Regression
lr = LogisticRegression(featuresCol='features_assembled', labelCol='class_index')

# Naive Bayes
nb = NaiveBayes(featuresCol='features_assembled', labelCol='class_index')

# Parameter grids for Logistic Regression with fitIntercept
lr_param_grid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.2, 0.5, 0.8]) \
    .addGrid(lr.maxIter, [50, 100, 150]) \
    .addGrid(lr.tol, [1e-06, 1e-05, 1e-04]) \
    .addGrid(lr.fitIntercept, [True, False]) \
    .build()

# Parameter grids for Naive Bayes
nb_param_grid = ParamGridBuilder() \
    .addGrid(nb.smoothing, [0.1, 0.5, 1.0, 1.5, 2.0]) \
    .addGrid(nb.modelType, ['multinomial', 'bernoulli']) \
    .build()

# Membuat evaluator
evaluator = BinaryClassificationEvaluator(labelCol='class_index')

# Membuat pipeline untuk Logistic Regression
lr_pipeline = Pipeline(stages=[assembler, lr])

# Binarizer untuk fitur jika menggunakan modelType='bernoulli'
binarizer = Binarizer(inputCol='features_assembled', outputCol='binarized_features')

# Naive Bayes dengan modelType='bernoulli'
nb_bernoulli = NaiveBayes(featuresCol='binarized_features', labelCol='class_index', modelType='bernoulli')

# Pipeline dengan binarizer untuk Naive Bayes
nb_pipeline = Pipeline(stages=[assembler, binarizer, nb_bernoulli])

# CrossValidator untuk Logistic Regression
lr_crossval = CrossValidator(estimator=lr_pipeline,
                             estimatorParamMaps=lr_param_grid,
                             evaluator=evaluator,
                             numFolds=5)

# CrossValidator untuk Naive Bayes
nb_crossval = CrossValidator(estimator=nb_pipeline,
                             estimatorParamMaps=nb_param_grid,
                             evaluator=evaluator,
                             numFolds=3)

# Melatih model Logistic Regression menggunakan CrossValidator
lr_cv_model = lr_crossval.fit(train)

# Melatih model Naive Bayes menggunakan CrossValidator
nb_cv_model = nb_crossval.fit(train)

# Mendapatkan model terbaik dari Logistic Regression
best_lr_model = lr_cv_model.bestModel

# Mendapatkan model terbaik dari Naive Bayes
best_nb_model = nb_cv_model.bestModel

# Menampilkan parameter terbaik yang diperoleh
best_lr_params = best_lr_model.stages[-1].extractParamMap()
print("Best Logistic Regression Parameters:")
for param, value in best_lr_params.items():
    print(f"{param.name}: {value}")

best_nb_params = best_nb_model.stages[-1].extractParamMap()
print("Best Naive Bayes Parameters:")
for param, value in best_nb_params.items():
    print(f"{param.name}: {value}")

# Fungsi untuk mengevaluasi model
def evaluate_model(model, data, evaluator):
    predictions = model.transform(data)
    roc_auc = evaluator.evaluate(predictions)
    return predictions, roc_auc

# Fungsi untuk menghitung metrik
def calculate_metrics(predictions):
    pred_array = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()
    label_array = [int(row.class_index) for row in predictions.select('class_index').collect()]

    accuracy = accuracy_score(label_array, pred_array) * 100
    precision = precision_score(label_array, pred_array) * 100
    recall = recall_score(label_array, pred_array) * 100
    f1 = f1_score(label_array, pred_array) * 100

    return accuracy, precision, recall, f1

# Fungsi untuk menghitung confusion matrix
def get_confusion_matrix(predictions):
    pred_array = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()
    label_array = [int(row.class_index) for row in predictions.select('class_index').collect()]
    return confusion_matrix(label_array, pred_array)

# Fungsi untuk mengevaluasi dan menghitung metrik untuk model
def evaluate_and_calculate_metrics(model, data):
    predictions, _ = evaluate_model(model, data, evaluator)
    metrics = calculate_metrics(predictions)
    confusion_mtx = get_confusion_matrix(predictions)
    return metrics, confusion_mtx

# Mengevaluasi model terbaik pada data validasi dan uji
lr_val_metrics, lr_val_confusion_mtx = evaluate_and_calculate_metrics(best_lr_model, validation)
nb_val_metrics, nb_val_confusion_mtx = evaluate_and_calculate_metrics(best_nb_model, validation)

lr_test_metrics, lr_test_confusion_mtx = evaluate_and_calculate_metrics(best_lr_model, test)
nb_test_metrics, nb_test_confusion_mtx = evaluate_and_calculate_metrics(best_nb_model, test)

# Menampilkan metrik evaluasi
def print_metrics(model_name, metrics, dataset_type):
    accuracy, precision, recall, f1 = metrics
    print(f'{model_name} ({dataset_type})')
    print(f'Accuracy: {accuracy:.2f}%')
    print(f'Precision: {precision:.2f}%')
    print(f'Recall: {recall:.2f}%')
    print(f'F1-Score: {f1:.2f}%')
    print()

print_metrics('Logistic Regression', lr_val_metrics, 'Validation')
print_metrics('Naive Bayes', nb_val_metrics, 'Validation')

print_metrics('Logistic Regression', lr_test_metrics, 'Test')
print_metrics('Naive Bayes', nb_test_metrics, 'Test')

# Plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(lr_val_confusion_mtx, 'Confusion Matrix Logistic Regression (Validation Hyperparameter Tuning)')
plot_confusion_matrix(nb_val_confusion_mtx, 'Confusion Matrix Naive Bayes (Validation Hyperparameter Tuning)')

plot_confusion_matrix(lr_test_confusion_mtx, 'Confusion Matrix Logistic Regression (Test Hyperparameter Tuning)')
plot_confusion_matrix(nb_test_confusion_mtx, 'Confusion Matrix Naive Bayes (Test Hyperparameter Tuning)')

"""# **ROC Logistic Regression & Naive Bayes**"""

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.functions import vector_to_array
from pyspark.ml.linalg import DenseVector
import numpy as np
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Fungsi untuk menghitung ROC AUC
def calculate_roc_auc(predictions):
    # Convert Spark DataFrame columns to arrays
    probabilities = predictions.select('probability').rdd.map(lambda row: row[0].toArray()[1]).collect()
    labels = [row.class_index for row in predictions.select('class_index').collect()]

    # Calculate ROC AUC
    auc = roc_auc_score(labels, probabilities)
    return auc * 100  # Convert to percentage

# Fungsi untuk plot ROC Curve dan simpan ke file
def plot_and_save_roc_curve(predictions, title, file_name):
    # Convert Spark DataFrame columns to arrays
    probabilities = predictions.select('probability').rdd.map(lambda row: row[0].toArray()[1]).collect()
    labels = [row.class_index for row in predictions.select('class_index').collect()]

    # Calculate ROC Curve
    fpr, tpr, _ = roc_curve(labels, probabilities)

    # Plot ROC Curve
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, marker='.')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.grid(True)

    # Save the plot to a file
    plt.savefig(file_name)
    plt.close()

# Evaluasi model untuk ROC AUC dan plot ROC Curve
def evaluate_and_plot_roc(model, data, model_name, file_prefix):
    predictions, _ = evaluate_model(model, data, evaluator)
    auc = calculate_roc_auc(predictions)
    print(f'{model_name} ROC AUC: {auc:.0f}%')

    # Plot and save ROC Curve for Validation and Test
    plot_and_save_roc_curve(predictions, f'ROC Curve for {model_name}', f'{file_prefix}_roc_curve.png')

# Mengevaluasi dan plot ROC untuk model terbaik
evaluate_and_plot_roc(best_lr_model, validation, 'Logistic Regression (Validation)', 'lr_validation')
evaluate_and_plot_roc(best_nb_model, validation, 'Naive Bayes (Validation)', 'nb_validation')

evaluate_and_plot_roc(best_lr_model, test, 'Logistic Regression (Test)', 'lr_test')
evaluate_and_plot_roc(best_nb_model, test, 'Naive Bayes (Test)', 'nb_test')

"""# **2. Decision Tree & Random Forest**"""

from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Menentukan nama kolom fitur yang benar
feature_columns = [col for col in train.columns if col != 'class_index']

# Assembler untuk fitur dengan nama kolom yang unik untuk setiap model
assembler_dt = VectorAssembler(inputCols=feature_columns, outputCol='features_dt')
assembler_rf = VectorAssembler(inputCols=feature_columns, outputCol='features_rf')

# Define Models
dt = DecisionTreeClassifier(featuresCol='features_dt', labelCol='class_index')
rf = RandomForestClassifier(featuresCol='features_rf', labelCol='class_index')

# Define Pipelines tanpa scaler
dt_pipeline = Pipeline(stages=[assembler_dt, dt])
rf_pipeline = Pipeline(stages=[assembler_rf, rf])

# Parameter grids for each model
dt_param_grid = ParamGridBuilder().addGrid(dt.maxDepth, [5, 10]).build()
rf_param_grid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).addGrid(rf.maxDepth, [5, 10]).build()

# Evaluator for ROC
evaluator_roc = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='class_index', metricName='areaUnderROC')

# Cross Validators for each model
dt_cv = CrossValidator(estimator=dt_pipeline, estimatorParamMaps=dt_param_grid, evaluator=evaluator_roc, numFolds=5)
rf_cv = CrossValidator(estimator=rf_pipeline, estimatorParamMaps=rf_param_grid, evaluator=evaluator_roc, numFolds=5)

# Fit models using CrossValidator
dt_cv_model = dt_cv.fit(train)
rf_cv_model = rf_cv.fit(train)

# Get best models
dt_best_model = dt_cv_model.bestModel
rf_best_model = rf_cv_model.bestModel

# Print best parameters
print("Decision Tree Best Parameters:")
print(dt_best_model.stages[-1].extractParamMap())

print("Random Forest Best Parameters:")
print(rf_best_model.stages[-1].extractParamMap())

# Function to evaluate model
def evaluate_model(model, data, evaluator):
    predictions = model.transform(data)
    roc_auc = evaluator.evaluate(predictions)
    return predictions, roc_auc

# Function to calculate metrics
def calculate_metrics(predictions):
    pred_array = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()
    label_array = [int(row.class_index) for row in predictions.select('class_index').collect()]

    confusion_matrix_df = predictions.select('class_index', 'prediction').groupBy('class_index', 'prediction').count().toPandas()
    confusion_matrix_pivot = confusion_matrix_df.pivot(index='class_index', columns='prediction', values='count').fillna(0)

    accuracy = accuracy_score(label_array, pred_array) * 100
    precision = precision_score(label_array, pred_array) * 100
    recall = recall_score(label_array, pred_array) * 100
    f1 = f1_score(label_array, pred_array) * 100

    return confusion_matrix_pivot, accuracy, precision, recall, f1

# Function to evaluate and calculate metrics for a model
def evaluate_and_calculate_metrics(model, data):
    predictions, _ = evaluate_model(model, data, evaluator_roc)
    metrics = calculate_metrics(predictions)
    return metrics

# Evaluate best models on validation and test datasets
rf_val_metrics = evaluate_and_calculate_metrics(rf_best_model, validation)
dt_val_metrics = evaluate_and_calculate_metrics(dt_best_model, validation)

rf_test_metrics = evaluate_and_calculate_metrics(rf_best_model, test)
dt_test_metrics = evaluate_and_calculate_metrics(dt_best_model, test)

# Print evaluation metrics
def print_metrics(model_name, metrics, dataset_type):
    conf_matrix, accuracy, precision, recall, f1 = metrics
    print(f'{model_name} ({dataset_type})')
    print(f'Accuracy: {accuracy:.2f}%')
    print(f'Precision: {precision:.2f}%')
    print(f'Recall: {recall:.2f}%')
    print(f'F1-Score: {f1:.2f}%')
    print()

print_metrics('Random Forest', rf_val_metrics, 'Validation')
print_metrics('Decision Tree', dt_val_metrics, 'Validation')

print_metrics('Random Forest', rf_test_metrics, 'Test')
print_metrics('Decision Tree', dt_test_metrics, 'Test')

# Plot confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(title)
    plt.show()

plot_confusion_matrix(rf_val_metrics[0], 'Confusion Matrix Random Forest (Validation Hyperparameter Tuning)')
plot_confusion_matrix(dt_val_metrics[0], 'Confusion Matrix Decision Tree (Validation Hyperparameter Tuning)')

plot_confusion_matrix(rf_test_metrics[0], 'Confusion Matrix Random Forest (Test Hyperparameter Tuning)')
plot_confusion_matrix(dt_test_metrics[0], 'Confusion Matrix Decision Tree (Test Hyperparameter Tuning)')

"""# **ROC Decision Tree & Random Forest**"""

# Evaluate ROC AUC for best models on validation and test datasets and convert to percentage
def evaluate_model_roc_auc(model, data, evaluator):
    predictions = model.transform(data)
    roc_auc = evaluator.evaluate(predictions)
    return roc_auc * 100  # Convert ROC AUC to percentage

# Evaluate ROC AUC
rf_val_roc_auc = evaluate_model_roc_auc(rf_best_model, validation, evaluator_roc)
dt_val_roc_auc = evaluate_model_roc_auc(dt_best_model, validation, evaluator_roc)

rf_test_roc_auc = evaluate_model_roc_auc(rf_best_model, test, evaluator_roc)
dt_test_roc_auc = evaluate_model_roc_auc(dt_best_model, test, evaluator_roc)

# Print ROC AUC percentages
def print_roc_auc(model_name, roc_auc, dataset_type):
    print(f'{model_name} ROC AUC ({dataset_type}): {roc_auc:.2f}%')

print_roc_auc('Random Forest', rf_val_roc_auc, 'Validation')
print_roc_auc('Decision Tree', dt_val_roc_auc, 'Validation')

print_roc_auc('Random Forest', rf_test_roc_auc, 'Test')
print_roc_auc('Decision Tree', dt_test_roc_auc, 'Test')